---
title: Local API Server
description: Build AI applications with Jan's OpenAI-compatible API server.
---

import { Aside, LinkCard } from '@astrojs/starlight/components';

Jan provides an OpenAI-compatible API server that runs entirely on your computer. Use the same API patterns you know from OpenAI, but with complete control over your models and data.

## Features

- **OpenAI-compatible** - Drop-in replacement for OpenAI API
- **Local models** - Run GGUF models via llama.cpp
- **Cloud models** - Proxy to OpenAI, Anthropic, and others
- **Privacy-first** - Local models never send data externally
- **No vendor lock-in** - Switch between providers seamlessly

## Quick Start

Start the server in **Settings > Local API Server** and make requests to `http://localhost:1337/v1`:

```bash
curl http://localhost:1337/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "MODEL_ID",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

## Documentation

- [**API Reference**](/api) - Interactive API documentation with Try It Out
- [**API Configuration**](./api-server) - Server settings, authentication, CORS
- [**Engine Settings**](./llama-cpp) - Configure llama.cpp for your hardware
- [**Server Settings**](./settings) - Advanced configuration options

<LinkCard
  title="API Reference"
  description="Learn how to get started with the API Reference"
  href="/api"
/>

## Integration Examples

### Continue (VS Code)
```json
{
  "models": [{
    "title": "Jan",
    "provider": "openai",
    "baseURL": "http://localhost:1337/v1",
    "apiKey": "YOUR_API_KEY",
    "model": "MODEL_ID"
  }]
}
```

### Python (OpenAI SDK)
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:1337/v1",
    api_key="YOUR_API_KEY"
)

response = client.chat.completions.create(
    model="MODEL_ID",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### JavaScript/TypeScript
```javascript
const response = await fetch('http://localhost:1337/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer YOUR_API_KEY'
  },
  body: JSON.stringify({
    model: 'MODEL_ID',
    messages: [{ role: 'user', content: 'Hello!' }]
  })
});
```

## Supported Endpoints

| Endpoint | Description |
|----------|-------------|
| `/v1/chat/completions` | Chat completions (streaming supported) |
| `/v1/models` | List available models |
| `/v1/models/{id}` | Get model information |

<Aside type="note">
Jan implements the core OpenAI API endpoints. Some advanced features like function calling depend on model capabilities.
</Aside>

## Why Use Jan's API?

**Privacy** - Your data stays on your machine with local models
**Cost** - No API fees for local model usage
**Control** - Choose your models, parameters, and hardware
**Flexibility** - Mix local and cloud models as needed

## Related Resources

- [Models Overview](/docs/jan/manage-models) - Available models
- [Data Storage](/docs/jan/data-folder) - Where Jan stores data
- [Troubleshooting](/docs/jan/troubleshooting) - Common issues
- [GitHub Repository](https://github.com/janhq/jan) - Source code
